{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikiiitb2013/ERA/blob/main/S7/ERA1S7Code6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmVi-m2cgEd"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "### Normalize needs tensor(not np array)\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "metadata": {
        "id": "9EegtbtYckyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4MAJ9uBfu7A",
        "outputId": "9075357d-fff5-4ea9-843a-3de57158f3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 330359247.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 112475110.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 158258384.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 23317660.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6WgVR8wfxA2",
        "outputId": "86f1e991-4f95-4415-f47b-71af54e62d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net2, self).__init__()\n",
        "    self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 26\n",
        "\n",
        "    # CONVOLUTION BLOCK 1\n",
        "    self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "    ) # output_size = 24\n",
        "\n",
        "\n",
        "    # TRANSITION BLOCK 1\n",
        "    self.mp1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
        "    self.convblock3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=10, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "    ) # output_size = 12\n",
        "\n",
        "    # CONVOLUTION BLOCK 2\n",
        "    self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "    ) # output_size = 10\n",
        "    self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 8\n",
        "\n",
        "    # TRANSITION BLOCK 2\n",
        "    # self.mp2 = nn.MaxPool2d(2, 2) # output_size = 7\n",
        "    self.convblock6 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(10),\n",
        "        nn.ReLU()\n",
        "    ) # output_size = 8\n",
        "\n",
        "    # OUTPUT BLOCK\n",
        "    self.convblock7 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(10),\n",
        "        nn.ReLU()\n",
        "    ) # output_size = 6\n",
        "\n",
        "    self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6) # 7>> 9... nn.AdaptiveAvgPool((1, 1))\n",
        "        ) # output_size = 1\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(self.convblock2(self.dropout(self.convblock1(x))))\n",
        "    x = self.mp1(x)\n",
        "    x = self.dropout(self.convblock3(x))\n",
        "    x = self.dropout(self.convblock5(self.dropout(self.convblock4(x))))\n",
        "    # x = self.mp2(x)\n",
        "    x = self.dropout(self.convblock6(x))\n",
        "    # x = self.convblock9(self.convblock8(self.convblock7(x)))\n",
        "    x = self.gap((self.convblock7(x)))\n",
        "\n",
        "    x = x.view(-1, 10) #1x1x10> 10\n",
        "    return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "-BC7lTSUfz0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "# model = Net().to(device)\n",
        "model = Net2().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMWnOqc6f3FR",
        "outputId": "3e0ee32b-0463-4378-b8ae-ad718d031965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "       BatchNorm2d-2           [-1, 10, 26, 26]              20\n",
            "              ReLU-3           [-1, 10, 26, 26]               0\n",
            "           Dropout-4           [-1, 10, 26, 26]               0\n",
            "            Conv2d-5           [-1, 10, 24, 24]             900\n",
            "       BatchNorm2d-6           [-1, 10, 24, 24]              20\n",
            "              ReLU-7           [-1, 10, 24, 24]               0\n",
            "           Dropout-8           [-1, 10, 24, 24]               0\n",
            "         MaxPool2d-9           [-1, 10, 12, 12]               0\n",
            "           Conv2d-10            [-1, 8, 12, 12]              80\n",
            "      BatchNorm2d-11            [-1, 8, 12, 12]              16\n",
            "             ReLU-12            [-1, 8, 12, 12]               0\n",
            "          Dropout-13            [-1, 8, 12, 12]               0\n",
            "           Conv2d-14           [-1, 10, 10, 10]             720\n",
            "      BatchNorm2d-15           [-1, 10, 10, 10]              20\n",
            "             ReLU-16           [-1, 10, 10, 10]               0\n",
            "          Dropout-17           [-1, 10, 10, 10]               0\n",
            "           Conv2d-18             [-1, 20, 8, 8]           1,800\n",
            "      BatchNorm2d-19             [-1, 20, 8, 8]              40\n",
            "             ReLU-20             [-1, 20, 8, 8]               0\n",
            "          Dropout-21             [-1, 20, 8, 8]               0\n",
            "           Conv2d-22             [-1, 10, 8, 8]             200\n",
            "      BatchNorm2d-23             [-1, 10, 8, 8]              20\n",
            "             ReLU-24             [-1, 10, 8, 8]               0\n",
            "          Dropout-25             [-1, 10, 8, 8]               0\n",
            "           Conv2d-26             [-1, 10, 6, 6]             900\n",
            "      BatchNorm2d-27             [-1, 10, 6, 6]              20\n",
            "             ReLU-28             [-1, 10, 6, 6]               0\n",
            "        AvgPool2d-29             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 4,846\n",
            "Trainable params: 4,846\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.53\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.55\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    # print(correct)\n",
        "    # print(processed)\n",
        "    # print(len(train_losses))\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "jKSFkYbFf4z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model =  Net().to(device)\n",
        "model =  Net2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzJGaqlef7wk",
        "outputId": "fb4e8481-54e5-43af-a9b3-4f78187b5a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.3702585697174072 Batch_id=468 Accuracy=80.46: 100%|██████████| 469/469 [00:21<00:00, 21.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.2913, Accuracy: 9457/10000 (94.57%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1810915321111679 Batch_id=468 Accuracy=94.92: 100%|██████████| 469/469 [00:18<00:00, 25.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1405, Accuracy: 9716/10000 (97.16%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2856422960758209 Batch_id=468 Accuracy=96.08: 100%|██████████| 469/469 [00:17<00:00, 27.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1170, Accuracy: 9730/10000 (97.30%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13832958042621613 Batch_id=468 Accuracy=96.62: 100%|██████████| 469/469 [00:17<00:00, 26.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0837, Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13250143826007843 Batch_id=468 Accuracy=96.83: 100%|██████████| 469/469 [00:19<00:00, 24.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0728, Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11680203676223755 Batch_id=468 Accuracy=97.11: 100%|██████████| 469/469 [00:18<00:00, 24.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0692, Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18592660129070282 Batch_id=468 Accuracy=97.22: 100%|██████████| 469/469 [00:19<00:00, 23.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0675, Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16417112946510315 Batch_id=468 Accuracy=97.38: 100%|██████████| 469/469 [00:17<00:00, 26.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0594, Accuracy: 9847/10000 (98.47%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07838206738233566 Batch_id=468 Accuracy=97.51: 100%|██████████| 469/469 [00:18<00:00, 24.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9861/10000 (98.61%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11759345978498459 Batch_id=468 Accuracy=97.62: 100%|██████████| 469/469 [00:21<00:00, 21.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0522, Accuracy: 9858/10000 (98.58%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07380115985870361 Batch_id=468 Accuracy=97.72: 100%|██████████| 469/469 [00:17<00:00, 26.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0480, Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09572678804397583 Batch_id=468 Accuracy=97.82: 100%|██████████| 469/469 [00:19<00:00, 23.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0471, Accuracy: 9862/10000 (98.62%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07425452768802643 Batch_id=468 Accuracy=97.78: 100%|██████████| 469/469 [00:18<00:00, 25.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0485, Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0775337740778923 Batch_id=468 Accuracy=97.86: 100%|██████████| 469/469 [00:17<00:00, 26.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0456, Accuracy: 9874/10000 (98.74%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09006748348474503 Batch_id=468 Accuracy=97.85: 100%|██████████| 469/469 [00:19<00:00, 24.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 9880/10000 (98.80%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = [t_items.item() for t_items in train_losses]\n",
        "len(t)"
      ],
      "metadata": {
        "id": "GYz3nPBjf9rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot(t)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc)\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")"
      ],
      "metadata": {
        "id": "9UtUbGXVglBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 -\n",
        "\n",
        "# \tTarget:    Added GAP and remove the last BIG kernel\n",
        "\n",
        "# \tResults:   #Params - 4.8k, Train accuracy - 97.86%, Test accuracy - 98.80%\n",
        "\n",
        "# \tAnalysis:   We see there is a downfall of accuracy in both train and test.\n",
        "# \t            Now, Adding Global Average Pooling reduces accuracy. This is wrong interpretation.\n",
        "# \t\t\t\tWe are comparing a 8.4k model with 4.8k model. Since we have reduced model capacity, a reduction in performance is expected.\n",
        "# \t\t\t\tBut we maintain the underfitting which is a good thing.\n",
        "# \t\t\t\tTherefore to make apples to apples comparison, we are going to increase capacity back(to the limit we are allowed) in next step\n",
        "# \t\t\t\tand then compare the accuracies."
      ],
      "metadata": {
        "id": "NGFSxuLJgnJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}